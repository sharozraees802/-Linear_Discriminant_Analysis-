{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3c5d3052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: /Users/classroomservices/Desktop/Winter/Machine Learning/Project/Code/venv/bin/pip: bad interpreter: /Users/classroomservices/Desktop/Winter/Code/venv/bin/python3.14: no such file or directory\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy scikit-learn openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c5d84161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# LDA (Phase 2 only)\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# 5 classifiers\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "45e7fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Paths\n",
    "# -------------------------\n",
    "DATA_ROOT = \"Datasets\"\n",
    "RESULTS_XLSX = \"/Users/classroomservices/Desktop/Winter/Machine Learning/Project/Code/Datasets/Results.xlsx\"   # your template file\n",
    "LABEL_COL = \"Label\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# -------------------------\n",
    "# CV (Outer must be 10)\n",
    "# -------------------------\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Inner tuning CV: not specified by prof -> 3 folds speeds up a lot\n",
    "inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# -------------------------\n",
    "# Parallelism (IMPORTANT)\n",
    "# -------------------------\n",
    "# If your run \"stucks\", set N_JOBS = 1 (most stable).\n",
    "# On Mac, too much parallelism can stall in nested CV.\n",
    "N_JOBS = 2  # try 2 first; if stuck -> set to 1\n",
    "\n",
    "# -------------------------\n",
    "# Random search iterations\n",
    "# -------------------------\n",
    "# More iterations = better tuning but slower.\n",
    "# Typical balanced: 15–25.\n",
    "N_ITER = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ce08e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_one_dataset(dataset_id: int):\n",
    "    \"\"\"\n",
    "    Loads Datasets/<id>/train.csv and test.csv, combines them.\n",
    "    Reason: some datasets have too few samples per class in train.csv\n",
    "    -> 10-fold stratified CV fails. Combining fixes that.\n",
    "    \"\"\"\n",
    "    train_path = os.path.join(DATA_ROOT, str(dataset_id), \"train.csv\")\n",
    "    test_path  = os.path.join(DATA_ROOT, str(dataset_id), \"test.csv\")\n",
    "\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df  = pd.read_csv(test_path)\n",
    "\n",
    "    df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "    if LABEL_COL not in df.columns:\n",
    "        raise ValueError(f\"'{LABEL_COL}' not found in dataset {dataset_id}\")\n",
    "\n",
    "    X = df.drop(columns=[LABEL_COL])\n",
    "    y = pd.Series(pd.factorize(df[LABEL_COL])[0], index=df.index)  # numeric labels\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4332ce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_paramdist(clf_name: str):\n",
    "    \"\"\"\n",
    "    Return (model, param_distributions)\n",
    "    Using RandomizedSearchCV -> use parameter distributions (lists).\n",
    "    We keep a reasonably rich search space (NOT tiny), but we don't try every combo.\n",
    "    \"\"\"\n",
    "\n",
    "    if clf_name == \"SVM\":\n",
    "        # cache_size improves speed for RBF kernels\n",
    "        model = SVC(cache_size=2000)\n",
    "        param_dist = {\n",
    "            \"clf__C\": [0.1, 1, 10, 100],\n",
    "            \"clf__kernel\": [\"rbf\", \"linear\"],\n",
    "            \"clf__gamma\": [\"scale\", \"auto\", 0.01, 0.1],\n",
    "        }\n",
    "        return model, param_dist\n",
    "\n",
    "    if clf_name == \"KNN\":\n",
    "        model = KNeighborsClassifier()\n",
    "        param_dist = {\n",
    "            \"clf__n_neighbors\": list(range(3, 22, 2)),  # 3..21 odd\n",
    "            \"clf__weights\": [\"uniform\", \"distance\"],\n",
    "            \"clf__metric\": [\"minkowski\", \"euclidean\", \"manhattan\"],\n",
    "        }\n",
    "        return model, param_dist\n",
    "\n",
    "    if clf_name == \"DT\":\n",
    "        model = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "        param_dist = {\n",
    "            \"clf__max_depth\": [None, 5, 10, 15, 20, 30],\n",
    "            \"clf__min_samples_split\": [2, 5, 10, 20],\n",
    "            \"clf__min_samples_leaf\": [1, 2, 4, 8],\n",
    "        }\n",
    "        return model, param_dist\n",
    "\n",
    "    if clf_name == \"RF\":\n",
    "        model = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "        param_dist = {\n",
    "            \"clf__n_estimators\": [100, 200, 300],\n",
    "            \"clf__max_depth\": [None, 10, 20, 30],\n",
    "            \"clf__min_samples_split\": [2, 5, 10],\n",
    "            \"clf__min_samples_leaf\": [1, 2, 4],\n",
    "        }\n",
    "        return model, param_dist\n",
    "\n",
    "    if clf_name == \"MLP\":\n",
    "        # early_stopping speeds up a lot\n",
    "        model = MLPClassifier(\n",
    "            max_iter=300,\n",
    "            early_stopping=True,\n",
    "            n_iter_no_change=10,\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "        param_dist = {\n",
    "            \"clf__hidden_layer_sizes\": [(50,), (100,), (150,), (50,50), (100,50)],\n",
    "            \"clf__alpha\": [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "            \"clf__learning_rate_init\": [1e-4, 1e-3, 1e-2],\n",
    "        }\n",
    "        return model, param_dist\n",
    "\n",
    "    raise ValueError(\"Unknown classifier: \" + clf_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8e731979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_params(best_params: dict, prefix: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract params of a given step (e.g., 'clf__', 'lda__') and format.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    for k, v in best_params.items():\n",
    "        if k.startswith(prefix):\n",
    "            parts.append(f\"{k.replace(prefix,'')}={v}\")\n",
    "    return \"; \".join(parts)\n",
    "\n",
    "def get_lda_n_components(best_params: dict):\n",
    "    return best_params.get(\"lda__n_components\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "39aa8f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_classifier(dataset_id: int, clf_name: str, use_lda: bool):\n",
    "    \"\"\"\n",
    "    Runs nested CV for ONE dataset + ONE classifier.\n",
    "\n",
    "    Phase 1 (Baseline): Imputer -> Scaler -> Classifier\n",
    "    Phase 2 (LDA):      Imputer -> Scaler -> LDA -> Classifier\n",
    "\n",
    "    Outer CV = 10 folds (required)\n",
    "    Inner CV = 3 folds (tuning)\n",
    "    RandomizedSearchCV = faster than GridSearchCV\n",
    "    \"\"\"\n",
    "    X, y = load_one_dataset(dataset_id)\n",
    "    model, param_dist = get_model_and_paramdist(clf_name)\n",
    "\n",
    "    # LDA: components <= classes-1\n",
    "    n_classes = len(np.unique(y))\n",
    "    max_comp = max(1, n_classes - 1)\n",
    "\n",
    "    # Build pipeline\n",
    "    steps = [\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    "    if use_lda:\n",
    "        steps.append((\"lda\", LinearDiscriminantAnalysis(solver=\"svd\")))\n",
    "    steps.append((\"clf\", model))\n",
    "\n",
    "    pipe = Pipeline(steps)\n",
    "\n",
    "    # Add LDA params to search space (Phase 2 only)\n",
    "    if use_lda:\n",
    "        param_dist = dict(param_dist)\n",
    "        param_dist.update({\n",
    "            \"lda__n_components\": list(range(1, max_comp + 1))\n",
    "        })\n",
    "\n",
    "    fold_rows = []\n",
    "    phase_name = \"PHASE 2 (LDA)\" if use_lda else \"PHASE 1 (Baseline)\"\n",
    "\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"✅ {phase_name} | Data {dataset_id} | {clf_name}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    for fold_idx, (tr_idx, va_idx) in enumerate(outer_cv.split(X, y), start=1):\n",
    "        print(f\"➡️ {clf_name} Fold {fold_idx}/10 running...\")\n",
    "\n",
    "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=pipe,\n",
    "            param_distributions=param_dist,\n",
    "            n_iter=N_ITER,\n",
    "            scoring=\"f1_macro\",\n",
    "            cv=inner_cv,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=N_JOBS,\n",
    "            refit=True\n",
    "        )\n",
    "\n",
    "        t0 = time.time()\n",
    "        search.fit(X_tr, y_tr)\n",
    "        fit_t = time.time() - t0\n",
    "\n",
    "        y_pred = search.predict(X_va)\n",
    "        acc = accuracy_score(y_va, y_pred)\n",
    "        f1m = f1_score(y_va, y_pred, average=\"macro\")\n",
    "\n",
    "        print(f\"✅ Done Fold {fold_idx} | Acc={acc:.4f} F1={f1m:.4f} | fit={fit_t:.1f}s\")\n",
    "\n",
    "        best = search.best_params_\n",
    "\n",
    "        fold_rows.append({\n",
    "            \"fold\": fold_idx,\n",
    "            \"accuracy\": float(acc),\n",
    "            \"f1_macro\": float(f1m),\n",
    "            \"best_params\": best,\n",
    "            \"clf_params_str\": format_params(best, \"clf__\"),\n",
    "            \"lda_params_str\": format_params(best, \"lda__\") if use_lda else \"\",\n",
    "            \"lda_n_components\": get_lda_n_components(best) if use_lda else None\n",
    "        })\n",
    "\n",
    "    return fold_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d195e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_block_start_row(dataset_id: int) -> int:\n",
    "    # Data1 at row 1, Data2 at row 13, ... (each dataset block is 12 rows)\n",
    "    return 1 + (dataset_id - 1) * 12\n",
    "\n",
    "def fold_row(dataset_id: int, fold_idx: int) -> int:\n",
    "    # Fold1 row is start+2; Fold10 row is start+11\n",
    "    return dataset_block_start_row(dataset_id) + 2 + (fold_idx - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "92d39975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_phase1_before(ws_before, dataset_id: int, res: dict):\n",
    "    \"\"\"\n",
    "    Writes into Sheet 'Before FS-DR':\n",
    "      B..K metrics (Acc,F1 for SVM,KNN,DT,RF,MLP)\n",
    "      L..P classifier params (one cell per classifier)\n",
    "    \"\"\"\n",
    "    clfs = [\"SVM\", \"KNN\", \"DT\", \"RF\", \"MLP\"]\n",
    "\n",
    "    for f in range(1, 11):\n",
    "        r = fold_row(dataset_id, f)\n",
    "\n",
    "        # Metrics start at column B=2\n",
    "        col = 2\n",
    "        for clf in clfs:\n",
    "            row = res[clf][f-1]\n",
    "            ws_before.cell(row=r, column=col).value   = round(row[\"accuracy\"], 4)\n",
    "            ws_before.cell(row=r, column=col+1).value = round(row[\"f1_macro\"], 4)\n",
    "            col += 2\n",
    "\n",
    "        # Params start at column L=12\n",
    "        colp = 12\n",
    "        for clf in clfs:\n",
    "            row = res[clf][f-1]\n",
    "            ws_before.cell(row=r, column=colp).value = row[\"clf_params_str\"]\n",
    "            colp += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cc427f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_phase2_after(ws_after, dataset_id: int, res: dict):\n",
    "    \"\"\"\n",
    "    Writes into Sheet 'After FS-DR':\n",
    "      B..K metrics\n",
    "      L = No. Selected Features (LDA components) -> stored as \"SVM=2;KNN=1;...\"\n",
    "      M = Features Name -> \"N/A (LDA)\"\n",
    "      N..R = per classifier: \"LDA(...); clf params\"\n",
    "    \"\"\"\n",
    "    clfs = [\"SVM\", \"KNN\", \"DT\", \"RF\", \"MLP\"]\n",
    "\n",
    "    for f in range(1, 11):\n",
    "        r = fold_row(dataset_id, f)\n",
    "\n",
    "        # Metrics B..K\n",
    "        col = 2\n",
    "        for clf in clfs:\n",
    "            row = res[clf][f-1]\n",
    "            ws_after.cell(row=r, column=col).value   = round(row[\"accuracy\"], 4)\n",
    "            ws_after.cell(row=r, column=col+1).value = round(row[\"f1_macro\"], 4)\n",
    "            col += 2\n",
    "\n",
    "        # L: No. Selected Features (one cell, so we store all)\n",
    "        comps = [f\"{clf}={res[clf][f-1]['lda_n_components']}\" for clf in clfs]\n",
    "        ws_after.cell(row=r, column=12).value = \";\".join(comps)\n",
    "\n",
    "        # M: Features Name\n",
    "        ws_after.cell(row=r, column=13).value = \"N/A (LDA)\"\n",
    "\n",
    "        # N..R: LDA params + clf params for each classifier\n",
    "        c = 14\n",
    "        for clf in clfs:\n",
    "            row = res[clf][f-1]\n",
    "            ws_after.cell(row=r, column=c).value = f\"LDA({row['lda_params_str']}); {row['clf_params_str']}\"\n",
    "            c += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9066bc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================== DATASET 1 ====================\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Datasets/1/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m res1 = {}\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m clf \u001b[38;5;129;01min\u001b[39;00m clfs:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     res1[clf] = \u001b[43mrun_one_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_lda\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m write_phase1_before(ws_before, dataset_id, res1)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Save checkpoint (so you never lose progress)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mrun_one_classifier\u001b[39m\u001b[34m(dataset_id, clf_name, use_lda)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_one_classifier\u001b[39m(dataset_id: \u001b[38;5;28mint\u001b[39m, clf_name: \u001b[38;5;28mstr\u001b[39m, use_lda: \u001b[38;5;28mbool\u001b[39m):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m    Runs nested CV for ONE dataset + ONE classifier.\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m \u001b[33;03m    RandomizedSearchCV = faster than GridSearchCV\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     X, y = \u001b[43mload_one_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     model, param_dist = get_model_and_paramdist(clf_name)\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# LDA: components <= classes-1\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mload_one_dataset\u001b[39m\u001b[34m(dataset_id)\u001b[39m\n\u001b[32m      7\u001b[39m train_path = os.path.join(DATA_ROOT, \u001b[38;5;28mstr\u001b[39m(dataset_id), \u001b[33m\"\u001b[39m\u001b[33mtrain.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m test_path  = os.path.join(DATA_ROOT, \u001b[38;5;28mstr\u001b[39m(dataset_id), \u001b[33m\"\u001b[39m\u001b[33mtest.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m train_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m test_df  = pd.read_csv(test_path)\n\u001b[32m     13\u001b[39m df = pd.concat([train_df, test_df], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Winter/Machine Learning/Project/Code/venv/lib/python3.14/site-packages/pandas/io/parsers/readers.py:873\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, skip_blank_lines, parse_dates, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    861\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    862\u001b[39m     dialect,\n\u001b[32m    863\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    869\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    870\u001b[39m )\n\u001b[32m    871\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Winter/Machine Learning/Project/Code/venv/lib/python3.14/site-packages/pandas/io/parsers/readers.py:300\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    297\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Winter/Machine Learning/Project/Code/venv/lib/python3.14/site-packages/pandas/io/parsers/readers.py:1645\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1642\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1644\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1645\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Winter/Machine Learning/Project/Code/venv/lib/python3.14/site-packages/pandas/io/parsers/readers.py:1904\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1902\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1903\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1904\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1911\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1915\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Winter/Machine Learning/Project/Code/venv/lib/python3.14/site-packages/pandas/io/common.py:926\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    922\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    923\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    924\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    935\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Datasets/1/train.csv'"
     ]
    }
   ],
   "source": [
    "# Load Excel template\n",
    "wb = openpyxl.load_workbook(RESULTS_XLSX)\n",
    "ws_before = wb[\"Before FS-DR\"]\n",
    "ws_after  = wb[\"After FS-DR\"]\n",
    "\n",
    "clfs = [\"SVM\", \"KNN\", \"DT\", \"RF\", \"MLP\"]\n",
    "\n",
    "for dataset_id in range(1, 17):\n",
    "    print(f\"\\n\\n==================== DATASET {dataset_id} ====================\")\n",
    "\n",
    "    # -------- PHASE 1 (Baseline) --------\n",
    "    res1 = {}\n",
    "    for clf in clfs:\n",
    "        res1[clf] = run_one_classifier(dataset_id, clf, use_lda=False)\n",
    "    write_phase1_before(ws_before, dataset_id, res1)\n",
    "\n",
    "    # Save checkpoint (so you never lose progress)\n",
    "    wb.save(\"Results_checkpoint.xlsx\")\n",
    "    print(\"✅ Saved checkpoint after Phase 1:\", \"Results_checkpoint.xlsx\")\n",
    "\n",
    "    # -------- PHASE 2 (LDA) --------\n",
    "    res2 = {}\n",
    "    for clf in clfs:\n",
    "        res2[clf] = run_one_classifier(dataset_id, clf, use_lda=True)\n",
    "    write_phase2_after(ws_after, dataset_id, res2)\n",
    "\n",
    "    # Save checkpoint again\n",
    "    wb.save(\"Results_checkpoint.xlsx\")\n",
    "    print(\"✅ Saved checkpoint after Phase 2:\", \"Results_checkpoint.xlsx\")\n",
    "\n",
    "# Final save\n",
    "final_name = \"Results_FINAL.xlsx\"\n",
    "wb.save(final_name)\n",
    "print(\"\\n✅ DONE. Final file saved as:\", final_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
