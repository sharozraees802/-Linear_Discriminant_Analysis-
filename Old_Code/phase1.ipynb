{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f62675b7",
   "metadata": {},
   "source": [
    "<h1>CELL 1 â€” Install (run once if needed)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c95e69ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: /Users/classroomservices/Desktop/Winter/Machine Learning/Project/Code/venv/bin/pip: bad interpreter: /Users/classroomservices/Desktop/Winter/Code/venv/bin/python3.14: no such file or directory\n"
     ]
    }
   ],
   "source": [
    "# If you see \"ModuleNotFoundError: No module named 'numpy' or 'sklearn'\",\n",
    "# run this cell, then restart the kernel/runtime.\n",
    "\n",
    "!pip install -q numpy scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686e151c",
   "metadata": {},
   "source": [
    "<h1>ðŸ§© CELL 2 â€” Imports & Global Settings</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aff2f5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Phase 1 (Baseline) Imports\n",
    "# =========================\n",
    "\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "# Dataset (example: Iris). Replace later with your instructor datasets.\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Model selection (CV + tuning)\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "# Pipeline prevents leakage: preprocessing is fit only on training folds\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Metrics required by the project\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# 5 required classifiers\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c59be7",
   "metadata": {},
   "source": [
    "<h1>ðŸ§© CELL 3 â€” Result Container (Clean Output)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b62c923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# Data structure to store model output\n",
    "# ===================================\n",
    "\n",
    "@dataclass\n",
    "class Phase1Result:\n",
    "    classifier_name: str\n",
    "    acc_mean: float\n",
    "    acc_std: float\n",
    "    f1_mean: float\n",
    "    f1_std: float\n",
    "    best_params_each_fold: List[Dict[str, Any]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14167194",
   "metadata": {},
   "source": [
    "<h1>ðŸ§© CELL 4 â€” Define Classifiers + Hyperparameter Grids (Grid Search)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "685dcb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Phase 1: Classifiers + Hyperparameter search space (Grid)\n",
    "# ============================================================\n",
    "# IMPORTANT:\n",
    "# - We tune ONLY classifier hyperparameters in Phase 1.\n",
    "# - No DR/FS methods are used here.\n",
    "# - Grids are small enough to run quickly on Iris, but valid.\n",
    "# - You can expand the grids later for stronger experiments.\n",
    "\n",
    "def get_models_and_grids(random_state: int = 42) -> Dict[str, Tuple[Any, Dict[str, List[Any]], bool]]:\n",
    "    \"\"\"\n",
    "    Returns dictionary:\n",
    "      name -> (estimator, param_grid, needs_scaling)\n",
    "\n",
    "    needs_scaling = True for SVM, kNN, MLP because they are distance/gradient-based.\n",
    "    For tree-based models scaling is not required.\n",
    "    \"\"\"\n",
    "\n",
    "    models = {}\n",
    "\n",
    "    # 1) SVM\n",
    "    models[\"SVM\"] = (\n",
    "        SVC(),\n",
    "        {\n",
    "            \"clf__kernel\": [\"linear\", \"rbf\"],\n",
    "            \"clf__C\": [0.1, 1, 10, 100],\n",
    "            \"clf__gamma\": [\"scale\", \"auto\"],  # used for rbf; safe if kernel=linear\n",
    "        },\n",
    "        True,\n",
    "    )\n",
    "\n",
    "    # 2) k-NN\n",
    "    models[\"kNN\"] = (\n",
    "        KNeighborsClassifier(),\n",
    "        {\n",
    "            \"clf__n_neighbors\": [3, 5, 7, 9, 11],\n",
    "            \"clf__weights\": [\"uniform\", \"distance\"],\n",
    "            \"clf__p\": [1, 2],  # 1=Manhattan, 2=Euclidean\n",
    "        },\n",
    "        True,\n",
    "    )\n",
    "\n",
    "    # 3) Decision Tree\n",
    "    models[\"DecisionTree\"] = (\n",
    "        DecisionTreeClassifier(random_state=random_state),\n",
    "        {\n",
    "            \"clf__criterion\": [\"gini\", \"entropy\"],\n",
    "            \"clf__max_depth\": [None, 2, 3, 4, 5, 8, 10],\n",
    "            \"clf__min_samples_split\": [2, 5, 10],\n",
    "            \"clf__min_samples_leaf\": [1, 2, 4],\n",
    "        },\n",
    "        False,\n",
    "    )\n",
    "\n",
    "    # 4) Random Forest\n",
    "    models[\"RandomForest\"] = (\n",
    "        RandomForestClassifier(random_state=random_state),\n",
    "        {\n",
    "            \"clf__n_estimators\": [100, 300],\n",
    "            \"clf__max_depth\": [None, 3, 5, 8, 12],\n",
    "            \"clf__min_samples_split\": [2, 5],\n",
    "            \"clf__min_samples_leaf\": [1, 2],\n",
    "            \"clf__max_features\": [\"sqrt\", \"log2\", None],\n",
    "        },\n",
    "        False,\n",
    "    )\n",
    "\n",
    "    # 5) MLP\n",
    "    models[\"MLP\"] = (\n",
    "        MLPClassifier(random_state=random_state, max_iter=700),\n",
    "        {\n",
    "            \"clf__hidden_layer_sizes\": [(50,), (100,), (50, 50)],\n",
    "            \"clf__alpha\": [1e-4, 1e-3, 1e-2],\n",
    "            \"clf__learning_rate_init\": [1e-3, 1e-2],\n",
    "        },\n",
    "        True,\n",
    "    )\n",
    "\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db0b0e5",
   "metadata": {},
   "source": [
    "<h1>ðŸ§© CELL 5 â€” Build Leakage-Safe Pipeline (Baseline)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ca6d0756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Pipeline builder (Baseline, Phase 1 only)\n",
    "# ==========================================\n",
    "# KEY POINT FOR LEAKAGE PREVENTION:\n",
    "# - StandardScaler is inside the pipeline.\n",
    "# - During CV, scaler is fit ONLY on training fold, not the test fold.\n",
    "# - This avoids using any information from test data.\n",
    "\n",
    "def build_baseline_pipeline(classifier: Any, needs_scaling: bool) -> Pipeline:\n",
    "    steps = []\n",
    "    if needs_scaling:\n",
    "        steps.append((\"scaler\", StandardScaler()))\n",
    "    steps.append((\"clf\", classifier))\n",
    "    return Pipeline(steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc4e8ff",
   "metadata": {},
   "source": [
    "<h1>ðŸ§© CELL 6 â€” Nested CV: 10-Fold Evaluation + Inner GridSearch (No Leakage)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "430761ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Phase 1 Core: Nested CV (Outer 10-fold + Inner tuning)\n",
    "# ==========================================================\n",
    "# WHY NESTED CV?\n",
    "# - Project requires hyperparameter tuning without leakage.\n",
    "# - If you tune using the whole dataset then cross-validate,\n",
    "#   you leak test-fold information into tuning.\n",
    "#\n",
    "# Our approach:\n",
    "# Outer loop (10-fold): estimates generalization performance\n",
    "# Inner loop (5-fold): selects best hyperparameters on training fold only\n",
    "#\n",
    "# This is the standard leakage-safe experimental design.\n",
    "\n",
    "def run_phase1_baseline_nested_cv(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    random_state: int = 42,\n",
    "    outer_splits: int = 10,\n",
    "    inner_splits: int = 5,\n",
    "    tuning_scoring: str = \"f1_macro\",   # tune using macro-F1 (multi-class friendly)\n",
    ") -> List[Phase1Result]:\n",
    "\n",
    "    models = get_models_and_grids(random_state=random_state)\n",
    "\n",
    "    outer_cv = StratifiedKFold(n_splits=outer_splits, shuffle=True, random_state=random_state)\n",
    "    inner_cv = StratifiedKFold(n_splits=inner_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    results: List[Phase1Result] = []\n",
    "\n",
    "    for name, (clf, grid, needs_scaling) in models.items():\n",
    "        acc_scores: List[float] = []\n",
    "        f1_scores: List[float] = []\n",
    "        best_params_folds: List[Dict[str, Any]] = []\n",
    "\n",
    "        # OUTER CV: performance estimation\n",
    "        for fold_id, (train_idx, test_idx) in enumerate(outer_cv.split(X, y), start=1):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            # Build baseline pipeline (scaler inside pipeline if needed)\n",
    "            pipe = build_baseline_pipeline(clf, needs_scaling)\n",
    "\n",
    "            # INNER CV: hyperparameter tuning ONLY on training fold\n",
    "            search = GridSearchCV(\n",
    "                estimator=pipe,\n",
    "                param_grid=grid,\n",
    "                scoring=tuning_scoring,\n",
    "                cv=inner_cv,\n",
    "                n_jobs=-1,\n",
    "                refit=True,  # retrain on full training fold using best params\n",
    "            )\n",
    "\n",
    "            search.fit(X_train, y_train)  # tuning happens here (training fold only)\n",
    "\n",
    "            # Evaluate on OUTER test fold (never seen during tuning)\n",
    "            y_pred = search.predict(X_test)\n",
    "\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "            acc_scores.append(acc)\n",
    "            f1_scores.append(f1)\n",
    "            best_params_folds.append(search.best_params_)\n",
    "\n",
    "            # Optional fold progress\n",
    "            # print(f\"{name} | Fold {fold_id}: acc={acc:.4f}, f1={f1:.4f}\")\n",
    "\n",
    "        results.append(\n",
    "            Phase1Result(\n",
    "                classifier_name=name,\n",
    "                acc_mean=float(np.mean(acc_scores)),\n",
    "                acc_std=float(np.std(acc_scores, ddof=1)),\n",
    "                f1_mean=float(np.mean(f1_scores)),\n",
    "                f1_std=float(np.std(f1_scores, ddof=1)),\n",
    "                best_params_each_fold=best_params_folds,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794fd044",
   "metadata": {},
   "source": [
    "<h1>ðŸ§© CELL 7 â€” Reporting: Results Table (Mean Â± Std)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0b08f580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Print results: mean Â± std\n",
    "# ============================\n",
    "\n",
    "def print_results_table(results: List[Phase1Result]) -> None:\n",
    "    header = f\"{'Classifier':<14} {'Accuracy (meanÂ±std)':<24} {'Macro-F1 (meanÂ±std)':<24}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    for r in results:\n",
    "        acc_text = f\"{r.acc_mean:.4f} Â± {r.acc_std:.4f}\"\n",
    "        f1_text  = f\"{r.f1_mean:.4f} Â± {r.f1_std:.4f}\"\n",
    "        print(f\"{r.classifier_name:<14} {acc_text:<24} {f1_text:<24}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d5bf43",
   "metadata": {},
   "source": [
    "<h1>ðŸ§© CELL 8 â€” Reporting: Best Hyperparameters (Frequency Summary)<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e573d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Best hyperparameters: frequency over folds\n",
    "# ==========================================\n",
    "# The project asks to report the best hyperparameters.\n",
    "# Since we do nested CV, each outer fold has its own best params.\n",
    "# We'll summarize how often each parameter set is selected.\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def summarize_best_params(results: List[Phase1Result], top_k: int = 3) -> None:\n",
    "    print(\"\\nBest hyperparameters (frequency across outer folds):\")\n",
    "\n",
    "    for r in results:\n",
    "        params_tuples = [tuple(sorted(p.items())) for p in r.best_params_each_fold]\n",
    "        counts = Counter(params_tuples)\n",
    "\n",
    "        print(f\"\\n{r.classifier_name}:\")\n",
    "        for params_tuple, freq in counts.most_common(top_k):\n",
    "            print(f\"  {freq:>2} folds -> {dict(params_tuple)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4317fe4",
   "metadata": {},
   "source": [
    "<h1>ðŸ§© CELL 9 â€” Load Dataset (Iris Example)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2af426bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Iris\n",
      "Shape X: (150, 4) | Classes: 3\n",
      "Class names: ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Load dataset (Iris example)\n",
    "# ==============================\n",
    "# Replace this later with your instructor datasets.\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(\"Dataset: Iris\")\n",
    "print(\"Shape X:\", X.shape, \"| Classes:\", len(np.unique(y)))\n",
    "print(\"Class names:\", iris.target_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65b6034",
   "metadata": {},
   "source": [
    "<h1>ðŸ§© CELL 10 â€” Run Phase 1 Baseline + Print Outputs</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3efb2604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Phase 1 Results (Baseline: No DR/FS) ===\n",
      "Classifier     Accuracy (meanÂ±std)      Macro-F1 (meanÂ±std)     \n",
      "----------------------------------------------------------------\n",
      "SVM            0.9600 Â± 0.0344          0.9596 Â± 0.0348         \n",
      "kNN            0.9600 Â± 0.0466          0.9592 Â± 0.0480         \n",
      "DecisionTree   0.9400 Â± 0.0584          0.9387 Â± 0.0600         \n",
      "RandomForest   0.9533 Â± 0.0549          0.9526 Â± 0.0560         \n",
      "MLP            0.9400 Â± 0.0734          0.9393 Â± 0.0735         \n",
      "\n",
      "Best hyperparameters (frequency across outer folds):\n",
      "\n",
      "SVM:\n",
      "   4 folds -> {'clf__C': 1, 'clf__gamma': 'scale', 'clf__kernel': 'linear'}\n",
      "   2 folds -> {'clf__C': 100, 'clf__gamma': 'scale', 'clf__kernel': 'linear'}\n",
      "   2 folds -> {'clf__C': 1, 'clf__gamma': 'scale', 'clf__kernel': 'rbf'}\n",
      "\n",
      "kNN:\n",
      "   3 folds -> {'clf__n_neighbors': 9, 'clf__p': 2, 'clf__weights': 'distance'}\n",
      "   2 folds -> {'clf__n_neighbors': 7, 'clf__p': 2, 'clf__weights': 'uniform'}\n",
      "   1 folds -> {'clf__n_neighbors': 11, 'clf__p': 2, 'clf__weights': 'uniform'}\n",
      "\n",
      "DecisionTree:\n",
      "   2 folds -> {'clf__criterion': 'gini', 'clf__max_depth': 4, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 2}\n",
      "   2 folds -> {'clf__criterion': 'gini', 'clf__max_depth': None, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 5}\n",
      "   2 folds -> {'clf__criterion': 'gini', 'clf__max_depth': None, 'clf__min_samples_leaf': 4, 'clf__min_samples_split': 2}\n",
      "\n",
      "RandomForest:\n",
      "   4 folds -> {'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 2, 'clf__n_estimators': 100}\n",
      "   2 folds -> {'clf__max_depth': 3, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 2, 'clf__min_samples_split': 2, 'clf__n_estimators': 100}\n",
      "   1 folds -> {'clf__max_depth': None, 'clf__max_features': None, 'clf__min_samples_leaf': 2, 'clf__min_samples_split': 5, 'clf__n_estimators': 100}\n",
      "\n",
      "MLP:\n",
      "   4 folds -> {'clf__alpha': 0.0001, 'clf__hidden_layer_sizes': (50,), 'clf__learning_rate_init': 0.001}\n",
      "   3 folds -> {'clf__alpha': 0.0001, 'clf__hidden_layer_sizes': (100,), 'clf__learning_rate_init': 0.01}\n",
      "   1 folds -> {'clf__alpha': 0.001, 'clf__hidden_layer_sizes': (50, 50), 'clf__learning_rate_init': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# =======================================\n",
    "# Run Phase 1 baseline experiments (Iris)\n",
    "# =======================================\n",
    "\n",
    "phase1_results = run_phase1_baseline_nested_cv(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    random_state=RANDOM_STATE,\n",
    "    outer_splits=10,\n",
    "    inner_splits=5,\n",
    "    tuning_scoring=\"f1_macro\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== Phase 1 Results (Baseline: No DR/FS) ===\")\n",
    "print_results_table(phase1_results)\n",
    "\n",
    "summarize_best_params(phase1_results, top_k=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
