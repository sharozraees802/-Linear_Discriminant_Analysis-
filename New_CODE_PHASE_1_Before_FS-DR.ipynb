{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d3052",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy scikit-learn openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5d84161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# LDA (Phase 2 only)\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# 5 classifiers\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e7fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Paths\n",
    "# -------------------------\n",
    "DATA_ROOT = \"Datasets\"\n",
    "RESULTS_XLSX = \"/Users/classroomservices/Desktop/Winter/Machine Learning/Project/Code/Datasets/Results.xlsx\"   # your template file\n",
    "LABEL_COL = \"Label\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# -------------------------\n",
    "# CV (Outer must be 10)\n",
    "# -------------------------\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Inner tuning CV: not specified by prof -> 3 folds speeds up a lot\n",
    "inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# -------------------------\n",
    "# Parallelism (IMPORTANT)\n",
    "# -------------------------\n",
    "# If your run \"stucks\", set N_JOBS = 1 (most stable).\n",
    "# On Mac, too much parallelism can stall in nested CV.\n",
    "N_JOBS = 2   # try 2 first; if stuck -> set to 1\n",
    "\n",
    "# -------------------------\n",
    "# Random search iterations\n",
    "# -------------------------\n",
    "# More iterations = better tuning but slower.\n",
    "# Typical balanced: 15–25.\n",
    "N_ITER = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce08e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_one_dataset(dataset_id: int):\n",
    "    \"\"\"\n",
    "    Loads Datasets/<id>/train.csv and test.csv, combines them.\n",
    "    Reason: some datasets have too few samples per class in train.csv\n",
    "    -> 10-fold stratified CV fails. Combining fixes that.\n",
    "    \"\"\"\n",
    "    train_path = os.path.join(DATA_ROOT, str(dataset_id), \"train.csv\")\n",
    "    test_path  = os.path.join(DATA_ROOT, str(dataset_id), \"test.csv\")\n",
    "\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df  = pd.read_csv(test_path)\n",
    "\n",
    "    df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "    if LABEL_COL not in df.columns:\n",
    "        raise ValueError(f\"'{LABEL_COL}' not found in dataset {dataset_id}\")\n",
    "\n",
    "    X = df.drop(columns=[LABEL_COL])\n",
    "    y = pd.Series(pd.factorize(df[LABEL_COL])[0], index=df.index)  # numeric labels\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4332ce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_paramdist(clf_name: str):\n",
    "    \"\"\"\n",
    "    Return (model, param_distributions)\n",
    "    Using RandomizedSearchCV -> use parameter distributions (lists).\n",
    "    We keep a reasonably rich search space (NOT tiny), but we don't try every combo.\n",
    "    \"\"\"\n",
    "\n",
    "    if clf_name == \"SVM\":\n",
    "        # cache_size improves speed for RBF kernels\n",
    "        model = SVC(cache_size=2000)\n",
    "        param_dist = {\n",
    "            \"clf__C\": [0.1, 1, 10, 100],\n",
    "            \"clf__kernel\": [\"rbf\", \"linear\"],\n",
    "            \"clf__gamma\": [\"scale\", \"auto\", 0.01, 0.1],\n",
    "        }\n",
    "        return model, param_dist\n",
    "\n",
    "    if clf_name == \"KNN\":\n",
    "        model = KNeighborsClassifier()\n",
    "        param_dist = {\n",
    "            \"clf__n_neighbors\": list(range(3, 22, 2)),  # 3..21 odd\n",
    "            \"clf__weights\": [\"uniform\", \"distance\"],\n",
    "            \"clf__metric\": [\"minkowski\", \"euclidean\", \"manhattan\"],\n",
    "        }\n",
    "        return model, param_dist\n",
    "\n",
    "    if clf_name == \"DT\":\n",
    "        model = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "        param_dist = {\n",
    "            \"clf__max_depth\": [None, 5, 10, 15, 20, 30],\n",
    "            \"clf__min_samples_split\": [2, 5, 10, 20],\n",
    "            \"clf__min_samples_leaf\": [1, 2, 4, 8],\n",
    "        }\n",
    "        return model, param_dist\n",
    "\n",
    "    if clf_name == \"RF\":\n",
    "        model = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "        param_dist = {\n",
    "            \"clf__n_estimators\": [100, 200, 300],\n",
    "            \"clf__max_depth\": [None, 10, 20, 30],\n",
    "            \"clf__min_samples_split\": [2, 5, 10],\n",
    "            \"clf__min_samples_leaf\": [1, 2, 4],\n",
    "        }\n",
    "        return model, param_dist\n",
    "\n",
    "    if clf_name == \"MLP\":\n",
    "        # early_stopping speeds up a lot\n",
    "        model = MLPClassifier(\n",
    "            max_iter=300,\n",
    "            early_stopping=True,\n",
    "            n_iter_no_change=10,\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "        param_dist = {\n",
    "            \"clf__hidden_layer_sizes\": [(50,), (100,), (150,), (50,50), (100,50)],\n",
    "            \"clf__alpha\": [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "            \"clf__learning_rate_init\": [1e-4, 1e-3, 1e-2],\n",
    "        }\n",
    "        return model, param_dist\n",
    "\n",
    "    raise ValueError(\"Unknown classifier: \" + clf_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e731979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_params(best_params: dict, prefix: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract params of a given step (e.g., 'clf__', 'lda__') and format.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    for k, v in best_params.items():\n",
    "        if k.startswith(prefix):\n",
    "            parts.append(f\"{k.replace(prefix,'')}={v}\")\n",
    "    return \"; \".join(parts)\n",
    "\n",
    "def get_lda_n_components(best_params: dict):\n",
    "    return best_params.get(\"lda__n_components\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39aa8f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_classifier(dataset_id: int, clf_name: str, use_lda: bool):\n",
    "    \"\"\"\n",
    "    Runs nested CV for ONE dataset + ONE classifier.\n",
    "\n",
    "    Phase 1 (Baseline): Imputer -> Scaler -> Classifier\n",
    "    Phase 2 (LDA):      Imputer -> Scaler -> LDA -> Classifier\n",
    "\n",
    "    Outer CV = 10 folds (required)\n",
    "    Inner CV = 3 folds (tuning)\n",
    "    RandomizedSearchCV = faster than GridSearchCV\n",
    "    \"\"\"\n",
    "    X, y = load_one_dataset(dataset_id)\n",
    "    model, param_dist = get_model_and_paramdist(clf_name)\n",
    "\n",
    "    # LDA: components <= classes-1\n",
    "    n_classes = len(np.unique(y))\n",
    "    max_comp = max(1, n_classes - 1)\n",
    "\n",
    "    # Build pipeline\n",
    "    steps = [\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    "    if use_lda:\n",
    "        steps.append((\"lda\", LinearDiscriminantAnalysis(solver=\"svd\")))\n",
    "    steps.append((\"clf\", model))\n",
    "\n",
    "    pipe = Pipeline(steps)\n",
    "\n",
    "    # Add LDA params to search space (Phase 2 only)\n",
    "    if use_lda:\n",
    "        param_dist = dict(param_dist)\n",
    "        param_dist.update({\n",
    "            \"lda__n_components\": list(range(1, max_comp + 1))\n",
    "        })\n",
    "\n",
    "    fold_rows = []\n",
    "    phase_name = \"PHASE 2 (LDA)\" if use_lda else \"PHASE 1 (Baseline)\"\n",
    "\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"✅ {phase_name} | Data {dataset_id} | {clf_name}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    for fold_idx, (tr_idx, va_idx) in enumerate(outer_cv.split(X, y), start=1):\n",
    "        print(f\"➡️ {clf_name} Fold {fold_idx}/10 running...\")\n",
    "\n",
    "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=pipe,\n",
    "            param_distributions=param_dist,\n",
    "            n_iter=N_ITER,\n",
    "            scoring=\"f1_macro\",\n",
    "            cv=inner_cv,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=N_JOBS,\n",
    "            refit=True\n",
    "        )\n",
    "\n",
    "        t0 = time.time()\n",
    "        search.fit(X_tr, y_tr)\n",
    "        fit_t = time.time() - t0\n",
    "\n",
    "        y_pred = search.predict(X_va)\n",
    "        acc = accuracy_score(y_va, y_pred)\n",
    "        f1m = f1_score(y_va, y_pred, average=\"macro\")\n",
    "\n",
    "        print(f\"✅ Done Fold {fold_idx} | Acc={acc:.4f} F1={f1m:.4f} | fit={fit_t:.1f}s\")\n",
    "\n",
    "        best = search.best_params_\n",
    "\n",
    "        fold_rows.append({\n",
    "            \"fold\": fold_idx,\n",
    "            \"accuracy\": float(acc),\n",
    "            \"f1_macro\": float(f1m),\n",
    "            \"best_params\": best,\n",
    "            \"clf_params_str\": format_params(best, \"clf__\"),\n",
    "            \"lda_params_str\": format_params(best, \"lda__\") if use_lda else \"\",\n",
    "            \"lda_n_components\": get_lda_n_components(best) if use_lda else None\n",
    "        })\n",
    "\n",
    "    return fold_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d195e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_block_start_row(dataset_id: int) -> int:\n",
    "    # Data1 at row 1, Data2 at row 13, ... (each dataset block is 12 rows)\n",
    "    return 1 + (dataset_id - 1) * 12\n",
    "\n",
    "def fold_row(dataset_id: int, fold_idx: int) -> int:\n",
    "    # Fold1 row is start+2; Fold10 row is start+11\n",
    "    return dataset_block_start_row(dataset_id) + 2 + (fold_idx - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92d39975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_phase1_before(ws_before, dataset_id: int, res: dict):\n",
    "    \"\"\"\n",
    "    Writes into Sheet 'Before FS-DR':\n",
    "      B..K metrics (Acc,F1 for SVM,KNN,DT,RF,MLP)\n",
    "      L..P classifier params (one cell per classifier)\n",
    "    \"\"\"\n",
    "    clfs = [\"SVM\", \"KNN\", \"DT\", \"RF\", \"MLP\"]\n",
    "\n",
    "    for f in range(1, 11):\n",
    "        r = fold_row(dataset_id, f)\n",
    "\n",
    "        # Metrics start at column B=2\n",
    "        col = 2\n",
    "        for clf in clfs:\n",
    "            row = res[clf][f-1]\n",
    "            ws_before.cell(row=r, column=col).value   = round(row[\"accuracy\"], 4)\n",
    "            ws_before.cell(row=r, column=col+1).value = round(row[\"f1_macro\"], 4)\n",
    "            col += 2\n",
    "\n",
    "        # Params start at column L=12\n",
    "        colp = 12\n",
    "        for clf in clfs:\n",
    "            row = res[clf][f-1]\n",
    "            ws_before.cell(row=r, column=colp).value = row[\"clf_params_str\"]\n",
    "            colp += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc427f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_phase2_after(ws_after, dataset_id: int, res: dict):\n",
    "    \"\"\"\n",
    "    Writes into Sheet 'After FS-DR':\n",
    "      B..K metrics\n",
    "      L = No. Selected Features (LDA components) -> stored as \"SVM=2;KNN=1;...\"\n",
    "      M = Features Name -> \"N/A (LDA)\"\n",
    "      N..R = per classifier: \"LDA(...); clf params\"\n",
    "    \"\"\"\n",
    "    clfs = [\"SVM\", \"KNN\", \"DT\", \"RF\", \"MLP\"]\n",
    "\n",
    "    for f in range(1, 11):\n",
    "        r = fold_row(dataset_id, f)\n",
    "\n",
    "        # Metrics B..K\n",
    "        col = 2\n",
    "        for clf in clfs:\n",
    "            row = res[clf][f-1]\n",
    "            ws_after.cell(row=r, column=col).value   = round(row[\"accuracy\"], 4)\n",
    "            ws_after.cell(row=r, column=col+1).value = round(row[\"f1_macro\"], 4)\n",
    "            col += 2\n",
    "\n",
    "        # L: No. Selected Features (one cell, so we store all)\n",
    "        comps = [f\"{clf}={res[clf][f-1]['lda_n_components']}\" for clf in clfs]\n",
    "        ws_after.cell(row=r, column=12).value = \";\".join(comps)\n",
    "\n",
    "        # M: Features Name\n",
    "        ws_after.cell(row=r, column=13).value = \"N/A (LDA)\"\n",
    "\n",
    "        # N..R: LDA params + clf params for each classifier\n",
    "        c = 14\n",
    "        for clf in clfs:\n",
    "            row = res[clf][f-1]\n",
    "            ws_after.cell(row=r, column=c).value = f\"LDA({row['lda_params_str']}); {row['clf_params_str']}\"\n",
    "            c += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9066bc43",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Results.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load Excel template\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m wb = \u001b[43mopenpyxl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_workbook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRESULTS_XLSX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m ws_before = wb[\u001b[33m\"\u001b[39m\u001b[33mBefore FS-DR\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      4\u001b[39m ws_after  = wb[\u001b[33m\"\u001b[39m\u001b[33mAfter FS-DR\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Winter/Machine Learning/Project/Code/venv/lib/python3.14/site-packages/openpyxl/reader/excel.py:346\u001b[39m, in \u001b[36mload_workbook\u001b[39m\u001b[34m(filename, read_only, keep_vba, data_only, keep_links, rich_text)\u001b[39m\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_workbook\u001b[39m(filename, read_only=\u001b[38;5;28;01mFalse\u001b[39;00m, keep_vba=KEEP_VBA,\n\u001b[32m    317\u001b[39m                   data_only=\u001b[38;5;28;01mFalse\u001b[39;00m, keep_links=\u001b[38;5;28;01mTrue\u001b[39;00m, rich_text=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    318\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Open the given filename and return the workbook\u001b[39;00m\n\u001b[32m    319\u001b[39m \n\u001b[32m    320\u001b[39m \u001b[33;03m    :param filename: the path to open or a file-like object\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    344\u001b[39m \n\u001b[32m    345\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     reader = \u001b[43mExcelReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_vba\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mdata_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_links\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrich_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    348\u001b[39m     reader.read()\n\u001b[32m    349\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m reader.wb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Winter/Machine Learning/Project/Code/venv/lib/python3.14/site-packages/openpyxl/reader/excel.py:123\u001b[39m, in \u001b[36mExcelReader.__init__\u001b[39m\u001b[34m(self, fn, read_only, keep_vba, data_only, keep_links, rich_text)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, read_only=\u001b[38;5;28;01mFalse\u001b[39;00m, keep_vba=KEEP_VBA,\n\u001b[32m    122\u001b[39m              data_only=\u001b[38;5;28;01mFalse\u001b[39;00m, keep_links=\u001b[38;5;28;01mTrue\u001b[39;00m, rich_text=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     \u001b[38;5;28mself\u001b[39m.archive = \u001b[43m_validate_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m     \u001b[38;5;28mself\u001b[39m.valid_files = \u001b[38;5;28mself\u001b[39m.archive.namelist()\n\u001b[32m    125\u001b[39m     \u001b[38;5;28mself\u001b[39m.read_only = read_only\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Winter/Machine Learning/Project/Code/venv/lib/python3.14/site-packages/openpyxl/reader/excel.py:95\u001b[39m, in \u001b[36m_validate_archive\u001b[39m\u001b[34m(filename)\u001b[39m\n\u001b[32m     88\u001b[39m             msg = (\u001b[33m'\u001b[39m\u001b[33mopenpyxl does not support \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m file format, \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     89\u001b[39m                    \u001b[33m'\u001b[39m\u001b[33mplease check you can open \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     90\u001b[39m                    \u001b[33m'\u001b[39m\u001b[33mit with Excel first. \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     91\u001b[39m                    \u001b[33m'\u001b[39m\u001b[33mSupported formats are: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m) % (file_format,\n\u001b[32m     92\u001b[39m                                                    \u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m.join(SUPPORTED_FORMATS))\n\u001b[32m     93\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidFileException(msg)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m archive = \u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m archive\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/zipfile/__init__.py:1453\u001b[39m, in \u001b[36mZipFile.__init__\u001b[39m\u001b[34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[39m\n\u001b[32m   1451\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m   1452\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1453\u001b[39m         \u001b[38;5;28mself\u001b[39m.fp = \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1454\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m   1455\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Results.xlsx'"
     ]
    }
   ],
   "source": [
    "# Load Excel template\n",
    "wb = openpyxl.load_workbook(RESULTS_XLSX)\n",
    "ws_before = wb[\"Before FS-DR\"]\n",
    "ws_after  = wb[\"After FS-DR\"]\n",
    "\n",
    "clfs = [\"SVM\", \"KNN\", \"DT\", \"RF\", \"MLP\"]\n",
    "\n",
    "for dataset_id in range(1, 17):\n",
    "    print(f\"\\n\\n==================== DATASET {dataset_id} ====================\")\n",
    "\n",
    "    # -------- PHASE 1 (Baseline) --------\n",
    "    res1 = {}\n",
    "    for clf in clfs:\n",
    "        res1[clf] = run_one_classifier(dataset_id, clf, use_lda=False)\n",
    "    write_phase1_before(ws_before, dataset_id, res1)\n",
    "\n",
    "    # Save checkpoint (so you never lose progress)\n",
    "    wb.save(\"Results_checkpoint.xlsx\")\n",
    "    print(\"✅ Saved checkpoint after Phase 1:\", \"Results_checkpoint.xlsx\")\n",
    "\n",
    "    # -------- PHASE 2 (LDA) --------\n",
    "    res2 = {}\n",
    "    for clf in clfs:\n",
    "        res2[clf] = run_one_classifier(dataset_id, clf, use_lda=True)\n",
    "    write_phase2_after(ws_after, dataset_id, res2)\n",
    "\n",
    "    # Save checkpoint again\n",
    "    wb.save(\"Results_checkpoint.xlsx\")\n",
    "    print(\"✅ Saved checkpoint after Phase 2:\", \"Results_checkpoint.xlsx\")\n",
    "\n",
    "# Final save\n",
    "final_name = \"Results_FINAL.xlsx\"\n",
    "wb.save(final_name)\n",
    "print(\"\\n✅ DONE. Final file saved as:\", final_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
